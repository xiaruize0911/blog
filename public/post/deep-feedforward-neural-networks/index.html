<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Feedforward Neural Networks | xiaruize's Blog</title><meta name=keywords content><meta name=description content="Deep Feedforward Neural Networks
This article is a study note for Chapter 6 of the book Deep Learning.
Introduction
Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.
From the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections."><meta name=author content><link rel=canonical href=https://xiaruize.org/post/deep-feedforward-neural-networks/><link crossorigin=anonymous href=/assets/css/stylesheet.e1f5c4cae44599655f7ff95195ff89c8cb3adbda94f2b1581a434ab2b4d4e6cf.css integrity="sha256-4fXEyuRFmWVff/lRlf+JyMs629qU8rFYGkNKsrTU5s8=" rel="preload stylesheet" as=style><link rel=icon href=https://xiaruize.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xiaruize.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xiaruize.org/favicon-32x32.png><link rel=apple-touch-icon href=https://xiaruize.org/apple-touch-icon.png><link rel=mask-icon href=https://xiaruize.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://xiaruize.org/post/deep-feedforward-neural-networks/><link rel=alternate hreflang=zh href=https://xiaruize.org/zh/post/deep-feedforward-neural-networks/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://xiaruize.org/post/deep-feedforward-neural-networks/"><meta property="og:site_name" content="xiaruize's Blog"><meta property="og:title" content="Deep Feedforward Neural Networks"><meta property="og:description" content="Deep Feedforward Neural Networks This article is a study note for Chapter 6 of the book Deep Learning.
Introduction Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.
From the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-09-22T22:40:11+00:00"><meta property="article:modified_time" content="2025-09-22T22:40:11+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Feedforward Neural Networks"><meta name=twitter:description content="Deep Feedforward Neural Networks
This article is a study note for Chapter 6 of the book Deep Learning.
Introduction
Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.
From the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://xiaruize.org/post/"},{"@type":"ListItem","position":2,"name":"Deep Feedforward Neural Networks","item":"https://xiaruize.org/post/deep-feedforward-neural-networks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Feedforward Neural Networks","name":"Deep Feedforward Neural Networks","description":"Deep Feedforward Neural Networks This article is a study note for Chapter 6 of the book Deep Learning.\nIntroduction Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.\nFrom the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections.\n","keywords":[],"articleBody":"Deep Feedforward Neural Networks This article is a study note for Chapter 6 of the book Deep Learning.\nIntroduction Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.\nFrom the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections.\nHere are some basic concepts:\nNeuron: The basic computational unit in a neural network, receives input and generates output. Each neuron can be regarded as a function that takes an input vector and outputs a scalar. Layer: A group of neurons in a neural network, usually divided by function into input layer, hidden layers, and output layer. Input Layer: The first layer of the network, receives external input data. Hidden Layers: The intermediate layers between the input and output layers. The number of hidden layers and the number of neurons in each layer are important design parameters. Output Layer: The last layer of the network, generates the final output result. Weights: Parameters of neurons that determine the influence of input data on the output. Weights are learned during the training process. Biases: Another parameter of neurons that allows the model to fit data more flexibly. Biases are also learned during training. ","wordCount":"242","inLanguage":"en","datePublished":"2025-09-22T22:40:11Z","dateModified":"2025-09-22T22:40:11Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaruize.org/post/deep-feedforward-neural-networks/"},"publisher":{"@type":"Organization","name":"xiaruize's Blog","logo":{"@type":"ImageObject","url":"https://xiaruize.org/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body id="
    top"><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaruize.org/ accesskey=h title="xiaruize's Blog (Alt + H)">xiaruize's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaruize.org/zh/ title=中文 aria-label=中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://xiaruize.org/categories/ title=categories><span>categories</span></a></li><li><a href=https://xiaruize.org/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Deep Feedforward Neural Networks</h1><div class=post-meta><span title='2025-09-22 22:40:11 +0000 UTC'>September 22, 2025</span>&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://xiaruize.org/zh/post/deep-feedforward-neural-networks/>Zh</a></li></ul></div></header><div class=post-content><h1 id=deep-feedforward-neural-networks>Deep Feedforward Neural Networks<a hidden class=anchor aria-hidden=true href=#deep-feedforward-neural-networks>#</a></h1><p>This article is a study note for Chapter 6 of the book <a href=http://www.deeplearningbook.org/>Deep Learning</a>.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Deep Feedforward Networks, also known as Multilayer Perceptrons (MLPs), are the most classic neural network models.</p><p>From the perspective of graph theory, a deep feedforward network is a Directed Acyclic Graph (DAG), where each node represents a neuron and each edge represents a connection weight. The connections between nodes are directional, and information can only flow along the direction of the edges. Therefore, deep feedforward networks have no cycles or feedback connections.</p><p>Here are some basic concepts:</p><ul><li><strong>Neuron</strong>: The basic computational unit in a neural network, receives input and generates output. Each neuron can be regarded as a function that takes an input vector and outputs a scalar.</li><li><strong>Layer</strong>: A group of neurons in a neural network, usually divided by function into input layer, hidden layers, and output layer.</li><li><strong>Input Layer</strong>: The first layer of the network, receives external input data.</li><li><strong>Hidden Layers</strong>: The intermediate layers between the input and output layers. The number of hidden layers and the number of neurons in each layer are important design parameters.</li><li><strong>Output Layer</strong>: The last layer of the network, generates the final output result.</li><li><strong>Weights</strong>: Parameters of neurons that determine the influence of input data on the output. Weights are learned during the training process.</li><li><strong>Biases</strong>: Another parameter of neurons that allows the model to fit data more flexibly. Biases are also learned during training.</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xiaruize.org/>xiaruize's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>