<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on xiaruize&#39;s Blog</title>
    <link>https://xiaruize.org/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on xiaruize&#39;s Blog</description>
    <generator>Hugo -- 0.150.0</generator>
    <language>en</language>
    <lastBuildDate>Thu, 25 Sep 2025 21:26:00 +0800</lastBuildDate>
    <atom:link href="https://xiaruize.org/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient Descent</title>
      <link>https://xiaruize.org/post/gradient-descent/</link>
      <pubDate>Thu, 25 Sep 2025 21:26:00 +0800</pubDate>
      <guid>https://xiaruize.org/post/gradient-descent/</guid>
      <description>&lt;h1 id=&#34;gradient-descent&#34;&gt;Gradient Descent&lt;/h1&gt;
&lt;p&gt;Gradient Descent is an iterative algorithm used to optimize objective functions, widely applied in machine learning and deep learning to train models by minimizing loss functions. It is recommended to first read the &lt;a href=&#34;https://xiaruize.org/machine-learning-basics&#34;&gt;Machine Learning Basics&lt;/a&gt; article for background understanding.&lt;/p&gt;
&lt;h2 id=&#34;objective-function&#34;&gt;Objective Function&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Objective Function&lt;/strong&gt;: Sometimes also called criterion, cost function, or loss function, is the function we aim to minimize, usually denoted as $J(\theta)$, where $\theta$ is the model parameter. For example, Mean Squared Error (MSE) used in linear regression is a common objective function.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
