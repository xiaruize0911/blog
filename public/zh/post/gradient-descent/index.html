<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Gradient Descent | xiaruize's Blog</title><meta name=keywords content><meta name=description content="Gradient Descent （梯度下降）
梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 机器学习基础 文章以对梯度下降的背景有大概的了解。
目标函数
目标函数 (Objective Function)：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\theta)$，其中 $\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。
一般来说，定义 $x^* = \arg\min_{x} J(x)$
导数
记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。
利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向
取一个小的正数 $\epsilon$，如果 $f'(x) > 0$，则 $f(x - \epsilon) < f(x)$，否则 $f(x + \epsilon) < f(x)$，也就是说：
$$
f(x -\epsilon\cdot \text{sign}(f'(x))) < f(x)
$$因此，导数的负方向是函数下降最快的方向。
梯度
下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \mathbb{R}^n \to \mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。"><meta name=author content><link rel=canonical href=https://xiaruize.org/zh/post/gradient-descent/><link crossorigin=anonymous href=/assets/css/stylesheet.e1f5c4cae44599655f7ff95195ff89c8cb3adbda94f2b1581a434ab2b4d4e6cf.css integrity="sha256-4fXEyuRFmWVff/lRlf+JyMs629qU8rFYGkNKsrTU5s8=" rel="preload stylesheet" as=style><link rel=icon href=https://xiaruize.org/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xiaruize.org/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xiaruize.org/favicon-32x32.png><link rel=apple-touch-icon href=https://xiaruize.org/apple-touch-icon.png><link rel=mask-icon href=https://xiaruize.org/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://xiaruize.org/zh/post/gradient-descent/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://xiaruize.org/zh/post/gradient-descent/"><meta property="og:site_name" content="xiaruize's Blog"><meta property="og:title" content="Gradient Descent"><meta property="og:description" content="Gradient Descent （梯度下降） 梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 机器学习基础 文章以对梯度下降的背景有大概的了解。
目标函数 目标函数 (Objective Function)：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\theta)$，其中 $\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。
一般来说，定义 $x^* = \arg\min_{x} J(x)$
导数 记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。
利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向
取一个小的正数 $\epsilon$，如果 $f'(x) > 0$，则 $f(x - \epsilon) < f(x)$，否则 $f(x + \epsilon) < f(x)$，也就是说：
$$ f(x -\epsilon\cdot \text{sign}(f'(x))) < f(x) $$因此，导数的负方向是函数下降最快的方向。
梯度 下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \mathbb{R}^n \to \mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-09-25T21:26:00+08:00"><meta property="article:modified_time" content="2025-09-25T21:26:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Gradient Descent"><meta name=twitter:description content="Gradient Descent （梯度下降）
梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 机器学习基础 文章以对梯度下降的背景有大概的了解。
目标函数
目标函数 (Objective Function)：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\theta)$，其中 $\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。
一般来说，定义 $x^* = \arg\min_{x} J(x)$
导数
记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。
利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向
取一个小的正数 $\epsilon$，如果 $f'(x) > 0$，则 $f(x - \epsilon) < f(x)$，否则 $f(x + \epsilon) < f(x)$，也就是说：
$$
f(x -\epsilon\cdot \text{sign}(f'(x))) < f(x)
$$因此，导数的负方向是函数下降最快的方向。
梯度
下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \mathbb{R}^n \to \mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://xiaruize.org/zh/post/"},{"@type":"ListItem","position":2,"name":"Gradient Descent","item":"https://xiaruize.org/zh/post/gradient-descent/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gradient Descent","name":"Gradient Descent","description":"Gradient Descent （梯度下降） 梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 机器学习基础 文章以对梯度下降的背景有大概的了解。\n目标函数 目标函数 (Objective Function)：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\\theta)$，其中 $\\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。\n一般来说，定义 $x^* = \\arg\\min_{x} J(x)$\n导数 记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。\n利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向\n取一个小的正数 $\\epsilon$，如果 $f'(x) \u003e 0$，则 $f(x - \\epsilon) \u003c f(x)$，否则 $f(x + \\epsilon) \u003c f(x)$，也就是说：\n$$ f(x -\\epsilon\\cdot \\text{sign}(f'(x))) \u003c f(x) $$因此，导数的负方向是函数下降最快的方向。\n梯度 下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \\ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。\n","keywords":[],"articleBody":"Gradient Descent （梯度下降） 梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 机器学习基础 文章以对梯度下降的背景有大概的了解。\n目标函数 目标函数 (Objective Function)：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\\theta)$，其中 $\\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。\n一般来说，定义 $x^* = \\arg\\min_{x} J(x)$\n导数 记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。\n利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向\n取一个小的正数 $\\epsilon$，如果 $f'(x) \u003e 0$，则 $f(x - \\epsilon) \u003c f(x)$，否则 $f(x + \\epsilon) \u003c f(x)$，也就是说：\n$$ f(x -\\epsilon\\cdot \\text{sign}(f'(x))) \u003c f(x) $$因此，导数的负方向是函数下降最快的方向。\n梯度 下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \\ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。\n在多维空间中，导数的概念被推广为梯度 (Gradient)，梯度是一个向量，包含了函数对每个变量的偏导数，表示为： $$ \\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]^T $$梯度的每个分量 $\\frac{\\partial f}{\\partial x_i}$ 表示函数 $f$ 对变量 $x_i$ 的变化率，梯度向量指示了函数在点 $x$ 处变化最快的方向。\n根据导数部分的结论，我们会认为应该跟着偏导数的方向进行更新。 但是，实际上，我们可以推出，对于一个小的正数 $\\epsilon$，$f(x)$ 下降最快的方向恰是 $-\\nabla f(x)$，所以有： $$ f(x - \\epsilon \\cdot \\nabla f(x)) \u003c f(x) $$梯度下降算法 基于上述结论，我们可以设计一个迭代算法来寻找函数的最小值，即每次沿着导数的负方向前进一个小步长，从而逐步逼近函数的最小值。\n根据导数的性质，$f'(x)=0$ 的点实际并不一定为极小值点。在实际情况下，一般不会进入鞍点（saddle point）。但是，我们找到的极小值点也不一定是全局最小值点，可能只是局部最小值点，且很有可能是局部最小值点。虽然这样，我们仍然接受局部最小值点，因为寻找全局最小值点通常是不可行的，且局部最小值点通常已经足够好。\n例如在这张图中，我们很难停在P2这个点上，因为浮点计算误差，很难精确的停在这个点上。但是有很大概率，我们会停在P1这个点上，即使它不是全局最小值点。通常这个点已经足够好了。\n扩展到多维空间中，我们可以设计如下的梯度下降算法：\n初始化参数 $x$，可以随机初始化或者使用某种启发式方法。 计算梯度 $\\nabla f(x)$。 更新参数：$x = x - \\alpha \\cdot \\nabla f(x)$，其中 $\\alpha$ 是学习率 (Learning Rate)，控制每次更新的步长。 重复步骤 2 和 3，直到满足停止条件（例如梯度足够小，或者达到最大迭代次数）。 Learning Rate (学习率) 学习率 $\\alpha$ 是梯度下降算法中的一个重要超参数，决定了每次参数更新的步长大小。选择合适的学习率对于梯度下降的收敛速度和最终结果有着显著影响。\n如果学习率过大，可能会导致参数更新过度，错过最优解，甚至使得目标函数发散。 如果学习率过小，收敛速度会非常慢，可能需要大量的迭代才能接近最优解，增加计算成本。 通常，学习率需要通过实验进行调优，可以使用学习率衰减（Learning Rate Decay）等技术来动态调整学习率，以提高收敛效果。在这里暂时不讨论这些技术。 ","wordCount":"162","inLanguage":"zh","datePublished":"2025-09-25T21:26:00+08:00","dateModified":"2025-09-25T21:26:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://xiaruize.org/zh/post/gradient-descent/"},"publisher":{"@type":"Organization","name":"xiaruize's Blog","logo":{"@type":"ImageObject","url":"https://xiaruize.org/favicon.ico"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!0})})</script></head><body id="
    top"><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xiaruize.org/zh/ accesskey=h title="xiaruize's Blog (Alt + H)">xiaruize's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://xiaruize.org/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://xiaruize.org/zh/categories/ title=categories><span>categories</span></a></li><li><a href=https://xiaruize.org/zh/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Gradient Descent</h1><div class=post-meta><span title='2025-09-25 21:26:00 +0800 CST'>2025年9月25日</span></div></header><div class=post-content><h1 id=gradient-descent-梯度下降>Gradient Descent （梯度下降）<a hidden class=anchor aria-hidden=true href=#gradient-descent-梯度下降>#</a></h1><p>梯度下降（Gradient Descent）是一种用于优化目标函数的迭代算法，广泛应用于机器学习和深度学习中，用于训练模型以最小化损失函数。推荐先阅读 <a href=/machine-learning-basics>机器学习基础</a> 文章以对梯度下降的背景有大概的了解。</p><h2 id=目标函数>目标函数<a hidden class=anchor aria-hidden=true href=#目标函数>#</a></h2><p><strong>目标函数 (Objective Function)</strong>：有时也称作准则(criterion)、代价函数(cost function)或损失函数(loss function)，是我们希望最小化的函数，通常表示为 $J(\theta)$，其中 $\theta$ 是模型的参数。例如，在线性回归中使用的 均方误差 (Mean Squared Error, MSE) 就是一个常见的目标函数。</p><p>一般来说，定义 $x^* = \arg\min_{x} J(x)$</p><h2 id=导数>导数<a hidden class=anchor aria-hidden=true href=#导数>#</a></h2><p>记现在需要优化的函数为 $y = f(x)$，这个函数的导数 $f'(x)$ 表示函数在点 $x$ 处的变化率，或者说是函数图像在该点的切线斜率。</p><p>利用导数的定义，发现导数实际指示了函数在该点的变化趋势，我们可以很容易的判断出函数下降的方向</p><p>取一个小的正数 $\epsilon$，如果 $f'(x) > 0$，则 $f(x - \epsilon) < f(x)$，否则 $f(x + \epsilon) < f(x)$，也就是说：</p>$$
f(x -\epsilon\cdot \text{sign}(f'(x))) < f(x)
$$<p>因此，导数的负方向是函数下降最快的方向。</p><h2 id=梯度>梯度<a hidden class=anchor aria-hidden=true href=#梯度>#</a></h2><p>下面，将上面所述的过程扩展到多维空间中，假设现在有一个多变量函数 $f: \mathbb{R}^n \to \mathbb{R}$，其输入为一个 $n$ 维向量 $x = [x_1, x_2, \ldots, x_n]^T$，输出为一个标量 $y = f(x)$。我们希望找到使得 $f(x)$ 最小化的 $x$。</p><p>在多维空间中，导数的概念被推广为梯度 (Gradient)，梯度是一个向量，包含了函数对每个变量的偏导数，表示为：</p>$$
\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right]^T
$$<p>梯度的每个分量 $\frac{\partial f}{\partial x_i}$ 表示函数 $f$ 对变量 $x_i$ 的变化率，梯度向量指示了函数在点 $x$ 处变化最快的方向。</p><p>根据导数部分的结论，我们会认为应该跟着偏导数的方向进行更新。
但是，实际上，我们可以推出，对于一个小的正数 $\epsilon$，$f(x)$ 下降最快的方向恰是 $-\nabla f(x)$，所以有：</p>$$
f(x - \epsilon \cdot \nabla f(x)) < f(x)
$$<h2 id=梯度下降算法>梯度下降算法<a hidden class=anchor aria-hidden=true href=#梯度下降算法>#</a></h2><p>基于上述结论，我们可以设计一个迭代算法来寻找函数的最小值，即每次沿着导数的负方向前进一个小步长，从而逐步逼近函数的最小值。</p><p>根据导数的性质，$f'(x)=0$ 的点实际并不一定为极小值点。在实际情况下，一般不会进入鞍点（saddle point）。但是，我们找到的极小值点也不一定是全局最小值点，可能只是局部最小值点，且很有可能是局部最小值点。虽然这样，我们仍然接受局部最小值点，因为寻找全局最小值点通常是不可行的，且局部最小值点通常已经足够好。</p><p>例如在这张图中，我们很难停在P2这个点上，因为浮点计算误差，很难精确的停在这个点上。但是有很大概率，我们会停在P1这个点上，即使它不是全局最小值点。通常这个点已经足够好了。</p><p><img alt=梯度下降示意图 loading=lazy src=1JjcQ.png></p><p>扩展到多维空间中，我们可以设计如下的梯度下降算法：</p><ol><li>初始化参数 $x$，可以随机初始化或者使用某种启发式方法。</li><li>计算梯度 $\nabla f(x)$。</li><li>更新参数：$x = x - \alpha \cdot \nabla f(x)$，其中 $\alpha$ 是学习率 (Learning Rate)，控制每次更新的步长。</li><li>重复步骤 2 和 3，直到满足停止条件（例如梯度足够小，或者达到最大迭代次数）。</li></ol><h2 id=learning-rate-学习率>Learning Rate (学习率)<a hidden class=anchor aria-hidden=true href=#learning-rate-学习率>#</a></h2><p>学习率 $\alpha$ 是梯度下降算法中的一个重要超参数，决定了每次参数更新的步长大小。选择合适的学习率对于梯度下降的收敛速度和最终结果有着显著影响。</p><ul><li>如果学习率过大，可能会导致参数更新过度，错过最优解，甚至使得目标函数发散。</li><li>如果学习率过小，收敛速度会非常慢，可能需要大量的迭代才能接近最优解，增加计算成本。
通常，学习率需要通过实验进行调优，可以使用学习率衰减（Learning Rate Decay）等技术来动态调整学习率，以提高收敛效果。在这里暂时不讨论这些技术。</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xiaruize.org/zh/>xiaruize's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>